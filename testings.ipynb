{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from capfourpy.authentication import get_access_token_interactive, get_azure_db_token_api\n",
    "from capfourpy.sharepoint import SharePoint\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import platform\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subclassing**: CustomSharePoint inherits from SharePoint, so it will have all the same methods and attributes.\n",
    "\n",
    "**Method Override**: By defining fetch_list_data within CustomSharePoint, it overrides the fetch_list_data method of the base SharePoint class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSharePoint(SharePoint):\n",
    "    \"\"\"\n",
    "    Subclass that inherits from capfourpy SharePoint\n",
    "    \"\"\"\n",
    "\n",
    "    # Class variable to store the token\n",
    "    idp_token = None\n",
    "\n",
    "    def _generate_token(self, token: str = \"missing idp token\"):\n",
    "        \"\"\"\n",
    "        Retrieves an authentication token using different methods based on the environment,\n",
    "        and caches it as a class-level variable to avoid repeated authentications.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        token (str, optional): Default token value, used when deployed. Defaults to \"missing idp token\".\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str: Authentication token for accessing SharePoint API.\n",
    "        \"\"\"\n",
    "        # Check if the token is already cached\n",
    "        if CustomSharePoint.idp_token is not None:\n",
    "            return CustomSharePoint.idp_token\n",
    "\n",
    "        # Generate the token - different methods for hosted and local\n",
    "        if platform.system() == \"Linux\":\n",
    "            try:\n",
    "                token = get_azure_db_token_api(scope=self.Scope)\n",
    "            except:\n",
    "                token = token  # Should always have a value when deployed, otherwise it will fail\n",
    "        else:\n",
    "            print(\"get_access_token_interactive\")\n",
    "            token = get_access_token_interactive(self.Client_Id, self.Tenant_Id, self.Scope)\n",
    "\n",
    "        # Cache the token at the class level\n",
    "        CustomSharePoint.idp_token = token\n",
    "        return token\n",
    "\n",
    "\n",
    "    def fetch_list_data(self, ListId: str = None, SiteUrl: str = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Retrieves all data from a specified SharePoint list and converts it to a DataFrame.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ListId (str, optional): The unique identifier of the SharePoint list to retrieve.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame: DataFrame containing all items from the specified SharePoint list.\n",
    "        \"\"\"\n",
    "        large_list = self.ctx.web.lists.get_by_id(list_id=ListId)\n",
    "\n",
    "        # items = large_list.items.get().execute_query()\n",
    "        items = large_list.items.get_all().execute_query()\n",
    "        # items = large_list.items.get_all().execute_query(500) # adding some number makes it run faster dunno why\n",
    "        data = [item.properties for item in items]\n",
    "\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    def list_files(self, folder_url: str):\n",
    "        \"\"\"\n",
    "        Retrieves a list of files in the specified SharePoint folder.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        folder_url : str\n",
    "            The server-relative URL of the target SharePoint folder.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List\n",
    "            List of files in the specified folder.\n",
    "        \"\"\"\n",
    "        folder = self.ctx.web.get_folder_by_server_relative_url(folder_url)\n",
    "\n",
    "        # Before executing the query\n",
    "        print(\"Before execute_query():\")\n",
    "        print(\"folder.properties:\", folder.properties)\n",
    "        print(\"Available methods and attributes:\", dir(folder))\n",
    "\n",
    "        # Expand and execute the query\n",
    "        folder.expand([\"Files\", \"Folders\"]).get().execute_query()\n",
    "\n",
    "\n",
    "        return folder.files\n",
    "\n",
    "\n",
    "    def get_files_metadata(self, folder_url: str):\n",
    "        \"\"\"\n",
    "        Retrieves files in the specified SharePoint folder along with their metadata.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        folder_url : str\n",
    "            The relative URL of the target SharePoint folder.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List\n",
    "            List of files with metadata in the specified folder.\n",
    "        \"\"\"\n",
    "        folder = self.ctx.web.get_folder_by_server_relative_url(folder_url)\n",
    "        # Expand to include ListItemAllFields to access metadata\n",
    "        files_metadata = folder.files.expand([\"ListItemAllFields\"]).get().execute_query()\n",
    "        return files_metadata\n",
    "\n",
    "\n",
    "    def get_page_content(self, page_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieves the content of a specified SharePoint page.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sp : SharePoint\n",
    "            An instance of the SharePoint class.\n",
    "        page_name : str\n",
    "            The name of the page (e.g., 'Home.aspx').\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The content of the page as a string.\n",
    "        \"\"\"\n",
    "        # Access the page using the ClientContext from the SharePoint instance\n",
    "        page = self.ctx.site_pages.pages.get_by_name(page_name).execute_query()\n",
    "\n",
    "        # Return the canvas content of the page\n",
    "        return page.canvas_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_access_token_interactive\n",
      "No accounts found in cache.\n",
      "No cached token found or expired. Initiating interactive authentication...\n"
     ]
    }
   ],
   "source": [
    "ListId = \"6ba7678f-2b65-4ad4-8759-21b68035c8c8\"\n",
    "SiteUrl = \"https://c4.sharepoint.com/sites/IMP\"\n",
    "\n",
    "sp = CustomSharePoint(site_url=SiteUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3796, 27)\n",
      "Index(['FileSystemObjectType', 'Id', 'ServerRedirectedEmbedUri',\n",
      "       'ServerRedirectedEmbedUrl', 'ID', 'ContentTypeId', 'Title', 'Modified',\n",
      "       'Created', 'AuthorId', 'EditorId', 'OData__UIVersionString',\n",
      "       'Attachments', 'GUID', 'ComplianceAssetId', 'EB_Created', 'EB_ID',\n",
      "       'EB_ListID', 'EB_SiteID', 'EB_SiteTitle', 'EB_SPWebUrl', 'EB_Url',\n",
      "       'FlowLog', 'EB_UniqueID', 'EB_Author', 'OData__ColorTag',\n",
      "       'EB_NoteType'],\n",
      "      dtype='object')\n",
      "[np.int64(0) np.int64(1) None '' np.int64(1)\n",
      " '0x0100F85F83B9B7B2164B971E241601880D090012394ABE830548438DEE171159507E12'\n",
      " 'Faurecia FY 2020' '2021-12-01T12:22:22Z' '2021-12-01T12:22:22Z'\n",
      " np.int64(1073741822) np.int64(1073741822) '1.0' np.False_\n",
      " '6aa973a0-d18c-4c15-9b4f-01f187e1f20d' None '2021-07-21T04:57:14Z'\n",
      " np.int64(12) 'cc3155c9-9c23-4de3-a701-624e2d25feab'\n",
      " 'b689d508-5290-4291-aa89-61470e5c2413' 'Faurecia'\n",
      " 'https://c4.sharepoint.com/sites/Faurecia73'\n",
      " 'https://c4.sharepoint.com/sites/Faurecia73/SitePages/Faurecia-FY-2020(1).aspx'\n",
      " None 'a2f533c1-1683-48b4-9d26-a712c114d2e3' 'Aske Taastrøm' None None]\n",
      "https://c4.sharepoint.com/sites/Faurecia73\n",
      "https://c4.sharepoint.com/sites/Faurecia73/SitePages/Faurecia-FY-2020(1).aspx\n"
     ]
    }
   ],
   "source": [
    "sp_data = sp.fetch_list_data(SiteUrl=SiteUrl, ListId=ListId)\n",
    "print(sp_data.shape)\n",
    "print(sp_data.columns)\n",
    "print(sp_data.iloc[0].values)\n",
    "print(sp_data.EB_SPWebUrl.values[0])\n",
    "print(sp_data.EB_Url.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store CustomSharePoint instances for each site URL to avoid redundant initializations\n",
    "sp_instances = {}\n",
    "\n",
    "def get_page_content_for_row(row):\n",
    "    \"\"\"\n",
    "    Retrieves the page content for a given row in the dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        A row from the dataframe containing 'EB_Url' and 'EB_SPWebUrl'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The content of the page.\n",
    "    \"\"\"\n",
    "    # Get the site URL from the 'EB_SPWebUrl' column\n",
    "    site_url = row['EB_SPWebUrl']\n",
    "\n",
    "    # Check if the site URL is valid\n",
    "    if pd.isna(site_url):\n",
    "        return ''\n",
    "\n",
    "    # Use or create a CustomSharePoint instance for this site URL\n",
    "    if site_url not in sp_instances:\n",
    "        sp_instances[site_url] = CustomSharePoint(site_url=site_url)\n",
    "    sp = sp_instances[site_url]\n",
    "\n",
    "    # Get the page URL from the 'EB_Url' column\n",
    "    EB_Url = row['EB_Url']\n",
    "\n",
    "    # Check if the page URL is valid\n",
    "    if pd.isna(EB_Url):\n",
    "        return ''\n",
    "\n",
    "    # Extract the page name from the page URL\n",
    "    site_pages_url = site_url.rstrip('/') + '/SitePages/'\n",
    "    if EB_Url.startswith(site_pages_url):\n",
    "        page_name = EB_Url[len(site_pages_url):]\n",
    "    else:\n",
    "        # Fallback method to extract the page name\n",
    "        parts = EB_Url.split('/SitePages/')\n",
    "        if len(parts) >= 2:\n",
    "            page_name = '/SitePages/'.join(parts[1:])\n",
    "        else:\n",
    "            print(parts, page_name)\n",
    "            # If unable to extract, return an empty string\n",
    "            return ''\n",
    "\n",
    "    # Retrieve the page content using the get_page_content method\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        page_content = sp.get_page_content(page_name)\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions and return an empty string\n",
    "        print(f\"Failed Extracting Page Content From Page: {page_name}\")\n",
    "        page_content = ''\n",
    "    return page_content\n",
    "\n",
    "\n",
    "def download_specific_files_for_row(row, desired_document_type, desired_document_subtype):\n",
    "    \"\"\"\n",
    "    Downloads files with specific metadata values from the '/Reorg/' document library for a given site.\n",
    "    Saves the files into the specified output_folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        A row from the dataframe containing 'EB_SPWebUrl'.\n",
    "    desired_document_type : str\n",
    "        The desired value for the \"Document Type\" column.\n",
    "    desired_document_subtype : str\n",
    "        The desired value for the \"Document SubType\" column.\n",
    "    output_folder : str\n",
    "        The folder path where files will be saved.\n",
    "    \"\"\"\n",
    "    site_url = row['EB_SPWebUrl']\n",
    "    rms_id = row['RmsId']\n",
    "    if pd.isna(site_url):\n",
    "        return\n",
    "\n",
    "    # Use or create a CustomSharePoint instance for this site URL\n",
    "    if site_url not in sp_instances:\n",
    "        sp_instances[site_url] = CustomSharePoint(site_url=site_url)\n",
    "    sp = sp_instances[site_url]\n",
    "\n",
    "    # Construct the server-relative URL for the '/Reorg/' folder\n",
    "    parsed_url = urlparse(site_url)\n",
    "    server_relative_url = parsed_url.path.rstrip('/') + '/Reorg/'\n",
    "\n",
    "    # Initialize a flag to check if the folder has been created\n",
    "    folder_created = False\n",
    "    files_downloaded = False  # Flag to check if any files were downloaded\n",
    "\n",
    "    # Get files with metadata in the folder\n",
    "    try:\n",
    "        files = sp.get_files_metadata(server_relative_url)\n",
    "        for file in files:\n",
    "            list_item_properties = file.listItemAllFields.properties\n",
    "            document_type = list_item_properties.get(\"DocumentType\", None)\n",
    "            document_subtype = list_item_properties.get(\"DocumentSubType\", None)\n",
    "            \n",
    "            if document_type == desired_document_type and document_subtype == desired_document_subtype:\n",
    "                # Get the file name\n",
    "                file_name = file.name\n",
    "\n",
    "                # Define the output folder path\n",
    "                output_folder = os.path.join('./sharepoint_reorg_files/', str(rms_id))\n",
    "\n",
    "                # Check if the file already exists\n",
    "                file_path = os.path.join(output_folder, file_name)\n",
    "                if not os.path.exists(file_path):\n",
    "                    # Create the folder if it hasn't been created yet\n",
    "                    if not folder_created:\n",
    "                        os.makedirs(output_folder, exist_ok=True)\n",
    "                        folder_created = True\n",
    "\n",
    "                    # Download the file\n",
    "                    file_url = file.serverRelativeUrl\n",
    "                    file_stream = sp.download_file(file_url)\n",
    "                    # Save the file into the specified output folder\n",
    "                    with open(file_path, 'wb') as f:\n",
    "                        f.write(file_stream.read())\n",
    "                    print(f\"Downloaded file {file_name} from {site_url} to {output_folder}\")\n",
    "                    files_downloaded = True\n",
    "                else:\n",
    "                    pass\n",
    "                    #print(f\"File {file_name} already exists in {output_folder}, skipping download.\")\n",
    "        if not files_downloaded:\n",
    "            print(f\"No new files to download for RmsId {rms_id} at {server_relative_url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download files from {server_relative_url}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Connection\n",
    "- **fundamental_score**: `CfRms_prod` Fundamental Scores Data\n",
    "- **rms_issuer**: `CfRms_prod` Linking RmsId to SharePoint Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RmsId</th>\n",
       "      <th>ScoringDate</th>\n",
       "      <th>CategoryGroup</th>\n",
       "      <th>Category</th>\n",
       "      <th>Score</th>\n",
       "      <th>TaggedCharacteristics</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>Status</th>\n",
       "      <th>SharePointLink</th>\n",
       "      <th>SharePointLinkTruncated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194</td>\n",
       "      <td>2021-04-15</td>\n",
       "      <td>Industry</td>\n",
       "      <td>Market Dynamics</td>\n",
       "      <td>2.0000000000</td>\n",
       "      <td>[{\"CharacteristicText\":\"Positive demographic, ...</td>\n",
       "      <td>Nexi</td>\n",
       "      <td>Active</td>\n",
       "      <td>https://c4.sharepoint.com/sites/194/</td>\n",
       "      <td>https://c4.sharepoint.com/sites/194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>194</td>\n",
       "      <td>2021-04-15</td>\n",
       "      <td>Industry</td>\n",
       "      <td>Intra-Industry Competition</td>\n",
       "      <td>2.0000000000</td>\n",
       "      <td>[{\"CharacteristicText\":\"Market share is consol...</td>\n",
       "      <td>Nexi</td>\n",
       "      <td>Active</td>\n",
       "      <td>https://c4.sharepoint.com/sites/194/</td>\n",
       "      <td>https://c4.sharepoint.com/sites/194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RmsId ScoringDate CategoryGroup                    Category         Score  \\\n",
       "0    194  2021-04-15      Industry             Market Dynamics  2.0000000000   \n",
       "1    194  2021-04-15      Industry  Intra-Industry Competition  2.0000000000   \n",
       "\n",
       "                               TaggedCharacteristics CompanyName  Status  \\\n",
       "0  [{\"CharacteristicText\":\"Positive demographic, ...        Nexi  Active   \n",
       "1  [{\"CharacteristicText\":\"Market share is consol...        Nexi  Active   \n",
       "\n",
       "                         SharePointLink              SharePointLinkTruncated  \n",
       "0  https://c4.sharepoint.com/sites/194/  https://c4.sharepoint.com/sites/194  \n",
       "1  https://c4.sharepoint.com/sites/194/  https://c4.sharepoint.com/sites/194  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from capfourpy.databases import Database\n",
    "\n",
    "# db = Database(database=\"C4DW\")\n",
    "# sql_query = \"SELECT * FROM DailyOverview.AssetData\"\n",
    "# data_database = db.read_sql(sql_query)\n",
    "\n",
    "\n",
    "# To retrieve data from the Azure database:\n",
    "db = Database(database=\"CfRms_prod\", azure=True)\n",
    "sql_query_fundamental_score = \"\"\"\n",
    "WITH tbl1 AS(\n",
    "\tSELECT r.ScoringId,\n",
    "\t\t   r.RmsId,\n",
    "\t\t   t.TemplateName AS ScoringType,\n",
    "\t\t   r.ScoringDate,\n",
    "\t\t   cat.Grouping AS CategoryGroup,\n",
    "\t\t   cat.Name AS Category,\n",
    "\t\t   rc.Score,\n",
    "\t\t   rc.Text,\n",
    "\t\t   (\n",
    "\t\t\t   SELECT c.Description AS CharacteristicText,\n",
    "\t\t\t\t\t  c.Influence AS CharacteristicInfluence\n",
    "\t\t\t   FROM Scoring.ResultCharacteristic AS rca\n",
    "\t\t\t\t   LEFT JOIN Scoring.Characteristic AS c ON c.CategoryId = rca.CategoryId AND c.CharacteristicId = rca.CharacteristicId\n",
    "\t\t\t   WHERE rca.ScoringId = rc.ScoringId AND rca.CategoryId = rc.CategoryId\n",
    "\t\t\t   FOR JSON PATH\n",
    "\t\t   ) AS TaggedCharacteristics\n",
    "\tFROM Scoring.Result AS r\n",
    "\t\tINNER JOIN Scoring.Template AS t ON t.TemplateId = r.TemplateId\n",
    "\t\tINNER JOIN Scoring.ResultCategory AS rc ON rc.ScoringId = r.ScoringId\n",
    "\t\tINNER JOIN Scoring.Category AS cat ON cat.CategoryId = rc.CategoryId\n",
    "\tWHERE t.TemplateName = 'Corporate'\n",
    ")\n",
    "SELECT * FROM tbl1 WHERE TaggedCharacteristics IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "sql_query_rms_issuer = \"\"\"\n",
    "SELECT *\n",
    "FROM [CfRms_prod].[Core].[RmsIssuer]\n",
    "WHERE SharePointLink IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "# Define dataframes from sql queries\n",
    "fundamental_score = db.read_sql(sql_query_fundamental_score)\n",
    "rms_issuer = db.read_sql(sql_query_rms_issuer)\n",
    "\n",
    "# Keep only relevant columns\n",
    "columns_to_remove_fundamental_score = ['ScoringId', 'ScoringType', 'Text']\n",
    "columns_to_remove_rms_issuer = ['PrimaryAnalystId', 'SecondaryAnalystId', 'ResearchTeam',\n",
    "       'CompanyDescription', 'BondTicker',\n",
    "       'OperatingCountryIso', 'Industry', 'Sponsor', 'MajorityOwnership',\n",
    "       'MinorityOwnership', 'WhyInvested', 'CreditPositives',\n",
    "       'CreditNegatives', 'CreditView', 'BookType', \n",
    "       'UpdateUser', 'SharePointExcelModel', 'SharePointSiteName',\n",
    "       'SharePointProvisioningStatus', 'SubIndustry',\n",
    "       'SharePointProvisioningMessage'] # 'Status',\n",
    "\n",
    "fundamental_score = fundamental_score.drop(columns=columns_to_remove_fundamental_score)\n",
    "rms_issuer = rms_issuer.drop(columns=columns_to_remove_rms_issuer)\n",
    "\n",
    "# Get a DataFrame with only names that have Fundamental Score\n",
    "rms_with_fundamental_score = fundamental_score.merge(rms_issuer, on='RmsId', how='left')\n",
    "rms_with_fundamental_score[\"SharePointLinkTruncated\"] = rms_with_fundamental_score[\"SharePointLink\"].apply(lambda x: x[:-1] if str(x).endswith('/') else x)\n",
    "rms_with_fundamental_score.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "668 620 522\n"
     ]
    }
   ],
   "source": [
    "# # Let's see how many SharePoint Sites we can link from our rms_with_fundamental_score to sp_data\n",
    "# unique_site_urls_sp_data = set(sp_data[\"EB_SPWebUrl\"])\n",
    "# unique_site_urls_rms_data = set(rms_with_fundamental_score[\"SharePointLinkTruncated\"])\n",
    "\n",
    "# count_unique_sp_data_site_urls = len(unique_site_urls_sp_data)\n",
    "# count_rms_site_urls = len(unique_site_urls_rms_data)\n",
    "# common_elements_count = len(unique_site_urls_sp_data.intersection(unique_site_urls_rms_data))\n",
    "\n",
    "# print(count_unique_sp_data_site_urls, count_rms_site_urls, common_elements_count)\n",
    "# unique_to_rms_site_urls = unique_site_urls_rms_data - unique_site_urls_sp_data\n",
    "# unique_to_rms_site_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pih\\AppData\\Local\\Temp\\ipykernel_19956\\1565844282.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sp_data_unique['EB_SPWebUrl_cleaned'] = sp_data_unique['EB_SPWebUrl'].astype(str).str.rstrip('/')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileSystemObjectType</th>\n",
       "      <th>Id</th>\n",
       "      <th>ServerRedirectedEmbedUri</th>\n",
       "      <th>ServerRedirectedEmbedUrl</th>\n",
       "      <th>ID</th>\n",
       "      <th>ContentTypeId</th>\n",
       "      <th>Title</th>\n",
       "      <th>Modified</th>\n",
       "      <th>Created</th>\n",
       "      <th>AuthorId</th>\n",
       "      <th>...</th>\n",
       "      <th>EB_Url</th>\n",
       "      <th>FlowLog</th>\n",
       "      <th>EB_UniqueID</th>\n",
       "      <th>EB_Author</th>\n",
       "      <th>OData__ColorTag</th>\n",
       "      <th>EB_NoteType</th>\n",
       "      <th>EB_SPWebUrl_cleaned</th>\n",
       "      <th>SharePointLinkTruncated_cleaned</th>\n",
       "      <th>RmsId</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0x0100F85F83B9B7B2164B971E241601880D090012394A...</td>\n",
       "      <td>Faurecia FY 2020</td>\n",
       "      <td>2021-12-01T12:22:22Z</td>\n",
       "      <td>2021-12-01T12:22:22Z</td>\n",
       "      <td>1073741822</td>\n",
       "      <td>...</td>\n",
       "      <td>https://c4.sharepoint.com/sites/Faurecia73/Sit...</td>\n",
       "      <td>None</td>\n",
       "      <td>a2f533c1-1683-48b4-9d26-a712c114d2e3</td>\n",
       "      <td>Aske Taastrøm</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://c4.sharepoint.com/sites/Faurecia73</td>\n",
       "      <td>https://c4.sharepoint.com/sites/Faurecia73</td>\n",
       "      <td>127</td>\n",
       "      <td>Active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>0x0100F85F83B9B7B2164B971E241601880D090012394A...</td>\n",
       "      <td>Techem - 3Q21 Results - Positive</td>\n",
       "      <td>2021-12-01T12:22:35Z</td>\n",
       "      <td>2021-12-01T12:22:35Z</td>\n",
       "      <td>1073741822</td>\n",
       "      <td>...</td>\n",
       "      <td>https://c4.sharepoint.com/sites/4692/SitePages...</td>\n",
       "      <td>None</td>\n",
       "      <td>4c6d3508-2663-4158-b37b-c1260b73f742</td>\n",
       "      <td>Andreas Dahl Jensen</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://c4.sharepoint.com/sites/4692</td>\n",
       "      <td>https://c4.sharepoint.com/sites/4692</td>\n",
       "      <td>287</td>\n",
       "      <td>Active</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   FileSystemObjectType  Id ServerRedirectedEmbedUri ServerRedirectedEmbedUrl  \\\n",
       "0                     0   1                     None                            \n",
       "7                     0   5                     None                            \n",
       "\n",
       "   ID                                      ContentTypeId  \\\n",
       "0   1  0x0100F85F83B9B7B2164B971E241601880D090012394A...   \n",
       "7   5  0x0100F85F83B9B7B2164B971E241601880D090012394A...   \n",
       "\n",
       "                              Title              Modified  \\\n",
       "0                  Faurecia FY 2020  2021-12-01T12:22:22Z   \n",
       "7  Techem - 3Q21 Results - Positive  2021-12-01T12:22:35Z   \n",
       "\n",
       "                Created    AuthorId  ...  \\\n",
       "0  2021-12-01T12:22:22Z  1073741822  ...   \n",
       "7  2021-12-01T12:22:35Z  1073741822  ...   \n",
       "\n",
       "                                              EB_Url FlowLog  \\\n",
       "0  https://c4.sharepoint.com/sites/Faurecia73/Sit...    None   \n",
       "7  https://c4.sharepoint.com/sites/4692/SitePages...    None   \n",
       "\n",
       "                            EB_UniqueID            EB_Author OData__ColorTag  \\\n",
       "0  a2f533c1-1683-48b4-9d26-a712c114d2e3        Aske Taastrøm            None   \n",
       "7  4c6d3508-2663-4158-b37b-c1260b73f742  Andreas Dahl Jensen            None   \n",
       "\n",
       "  EB_NoteType                         EB_SPWebUrl_cleaned  \\\n",
       "0        None  https://c4.sharepoint.com/sites/Faurecia73   \n",
       "7        None        https://c4.sharepoint.com/sites/4692   \n",
       "\n",
       "              SharePointLinkTruncated_cleaned RmsId  Status  \n",
       "0  https://c4.sharepoint.com/sites/Faurecia73   127  Active  \n",
       "7        https://c4.sharepoint.com/sites/4692   287  Active  \n",
       "\n",
       "[2 rows x 31 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_data_unique = sp_data.drop_duplicates(subset='EB_SPWebUrl')\n",
    "# sp_data_unique_50 = sp_data_unique.head(50)\n",
    "\n",
    "# Clean and prepare URLs for matching\n",
    "sp_data_unique['EB_SPWebUrl_cleaned'] = sp_data_unique['EB_SPWebUrl'].astype(str).str.rstrip('/')\n",
    "rms_with_fundamental_score['SharePointLinkTruncated_cleaned'] = rms_with_fundamental_score['SharePointLinkTruncated'].astype(str).str.rstrip('/')\n",
    "\n",
    "# Merge dataframes on cleaned URLs\n",
    "merged_data = sp_data_unique.merge(\n",
    "    rms_with_fundamental_score[['SharePointLinkTruncated_cleaned', 'RmsId', 'Status']],\n",
    "    left_on='EB_SPWebUrl_cleaned',\n",
    "    right_on='SharePointLinkTruncated_cleaned',\n",
    "    how='left')\n",
    "\n",
    "# Cast RMS\n",
    "merged_data = merged_data.dropna(subset=[\"RmsId\"])\n",
    "merged_data[\"RmsId\"] = merged_data[\"RmsId\"].astype(int)\n",
    "\n",
    "# Remove duplicates if necessary\n",
    "merged_data_unique = merged_data.drop_duplicates(subset='EB_SPWebUrl_cleaned')\n",
    "merged_data_unique.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Offerings (Prospectuses) From SharePoint Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new files to download for RmsId 127 at /sites/Faurecia73/Reorg/\n",
      "No new files to download for RmsId 287 at /sites/4692/Reorg/\n",
      "No new files to download for RmsId 12 at /sites/12/Reorg/\n",
      "No new files to download for RmsId 312 at /sites/Axilone77/Reorg/\n",
      "No new files to download for RmsId 135 at /sites/135/Reorg/\n",
      "No new files to download for RmsId 10 at /sites/109/Reorg/\n",
      "No new files to download for RmsId 20 at /sites/2077/Reorg/\n",
      "No new files to download for RmsId 120 at /sites/1209/Reorg/\n",
      "No new files to download for RmsId 74 at /sites/7477/Reorg/\n",
      "No new files to download for RmsId 327 at /sites/3272/Reorg/\n",
      "No new files to download for RmsId 451 at /sites/4512/Reorg/\n",
      "No new files to download for RmsId 134 at /sites/1349/Reorg/\n",
      "No new files to download for RmsId 199 at /sites/199/Reorg/\n",
      "No new files to download for RmsId 204 at /sites/204/Reorg/\n",
      "No new files to download for RmsId 171 at /sites/1712/Reorg/\n",
      "No new files to download for RmsId 201 at /sites/2019/Reorg/\n",
      "No new files to download for RmsId 274 at /sites/27477/Reorg/\n",
      "No new files to download for RmsId 280 at /sites/2802/Reorg/\n",
      "No new files to download for RmsId 291 at /sites/2912/Reorg/\n",
      "No new files to download for RmsId 299 at /sites/2999/Reorg/\n",
      "No new files to download for RmsId 177 at /sites/Limacorporate2/Reorg/\n",
      "No new files to download for RmsId 311 at /sites/311/Reorg/\n",
      "No new files to download for RmsId 432 at /sites/432/Reorg/\n",
      "No new files to download for RmsId 128 at /sites/502/Reorg/\n",
      "No new files to download for RmsId 358 at /sites/472/Reorg/\n",
      "No new files to download for RmsId 79 at /sites/79/Reorg/\n",
      "No new files to download for RmsId 56 at /sites/56/Reorg/\n",
      "No new files to download for RmsId 281 at /sites/281/Reorg/\n",
      "No new files to download for RmsId 17 at /sites/17/Reorg/\n",
      "No new files to download for RmsId 1 at /sites/1559/Reorg/\n",
      "No new files to download for RmsId 613 at /sites/613/Reorg/\n",
      "No new files to download for RmsId 130 at /sites/130559/Reorg/\n",
      "No new files to download for RmsId 246 at /sites/246/Reorg/\n",
      "No new files to download for RmsId 616 at /sites/6162/Reorg/\n",
      "No new files to download for RmsId 34 at /sites/34/Reorg/\n",
      "No new files to download for RmsId 32 at /sites/322/Reorg/\n",
      "No new files to download for RmsId 615 at /sites/6152/Reorg/\n",
      "No new files to download for RmsId 372 at /sites/372/Reorg/\n",
      "No new files to download for RmsId 258 at /sites/258/Reorg/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Skip rows where there is no matching RmsId\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Download files; the function will create the folder only if files are downloaded\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mdownload_specific_files_for_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesired_document_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesired_document_subtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 98\u001b[0m, in \u001b[0;36mdownload_specific_files_for_row\u001b[1;34m(row, desired_document_type, desired_document_subtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Get files with metadata in the folder\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_files_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_relative_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m    100\u001b[0m         list_item_properties \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mlistItemAllFields\u001b[38;5;241m.\u001b[39mproperties\n",
      "Cell \u001b[1;32mIn[16], line 107\u001b[0m, in \u001b[0;36mCustomSharePoint.get_files_metadata\u001b[1;34m(self, folder_url)\u001b[0m\n\u001b[0;32m    105\u001b[0m folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39mweb\u001b[38;5;241m.\u001b[39mget_folder_by_server_relative_url(folder_url)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Expand to include ListItemAllFields to access metadata\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m files_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mfolder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mListItemAllFields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m files_metadata\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\capfourpy\\lib\\site-packages\\office365\\runtime\\client_object.py:55\u001b[0m, in \u001b[0;36mClientObject.execute_query\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_query\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m# type: () -> Self\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Submit request(s) to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\capfourpy\\lib\\site-packages\\office365\\runtime\\client_runtime_context.py:173\u001b[0m, in \u001b[0;36mClientRuntimeContext.execute_query\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_pending_request:\n\u001b[0;32m    172\u001b[0m     qry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_query()\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpending_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqry\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\capfourpy\\lib\\site-packages\\office365\\runtime\\client_request.py:37\u001b[0m, in \u001b[0;36mClientRequest.execute_query\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(query)\n\u001b[1;32m---> 37\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_request_direct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response(response, query)\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafterExecute\u001b[38;5;241m.\u001b[39mnotify(response)\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\capfourpy\\lib\\site-packages\\office365\\runtime\\client_request.py:93\u001b[0m, in \u001b[0;36mClientRequest.execute_request_direct\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     84\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mput(\n\u001b[0;32m     85\u001b[0m         url\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl,\n\u001b[0;32m     86\u001b[0m         data\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mdata,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mproxies,\n\u001b[0;32m     91\u001b[0m     )\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 93\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\capfourpy\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\capfourpy\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\capfourpy\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\capfourpy\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\capfourpy\\lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\capfourpy\\lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\capfourpy\\lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\capfourpy\\lib\\site-packages\\urllib3\\connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\capfourpy\\lib\\http\\client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\capfourpy\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\capfourpy\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\capfourpy\\lib\\socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\capfourpy\\lib\\ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\capfourpy\\lib\\ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the desired metadata values\n",
    "desired_document_type = \"Legal\"\n",
    "desired_document_subtype = \"Offerings\"\n",
    "\n",
    "# Create a dictionary to store CustomSharePoint instances\n",
    "sp_instances = {}\n",
    "\n",
    "# Process each row in the merged dataframe\n",
    "for index, row in merged_data_unique.iterrows():\n",
    "    if pd.isna(row['RmsId']):\n",
    "        continue  # Skip rows where there is no matching RmsId\n",
    "    # Download files; the function will create the folder only if files are downloaded\n",
    "    download_specific_files_for_row(row, desired_document_type, desired_document_subtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the list of RmsId that does not have Prospectus\n",
    "sorted_rms_id_df = merged_data_unique[[\"RmsId\"]].sort_values(by=\"RmsId\").reset_index(drop=True)\n",
    "rms_id_list = sorted_rms_id_df[\"RmsId\"].to_list()\n",
    "\n",
    "directory_path = \"./sharepoint_reorg_files/\"\n",
    "# Get list of RmsId folders that actually exist in the directory\n",
    "existing_folders = [int(folder) for folder in os.listdir(directory_path) if folder.isdigit() and int(folder) in rms_id_list]\n",
    "\n",
    "# List of RmsId that do not have an associated folder\n",
    "rms_id_without_folders = [rms_id for rms_id in rms_id_list if rms_id not in existing_folders]\n",
    "#print(rms_id_without_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: Quarterly Earnings Report 2019Q2.pdf\n",
      "FileSystemObjectType: 0\n",
      "Id: 50\n",
      "ServerRedirectedEmbedUri: https://c4.sharepoint.com/sites/Faurecia73/_layouts/15/Embed.aspx?UniqueId=122ec8ae-40e3-48b9-b70b-00349c6ad3dd\n",
      "ServerRedirectedEmbedUrl: https://c4.sharepoint.com/sites/Faurecia73/_layouts/15/Embed.aspx?UniqueId=122ec8ae-40e3-48b9-b70b-00349c6ad3dd\n",
      "ContentTypeId: 0x010100A706DF5B1491F543B0F4A4F85B823691\n",
      "OData__ColorTag: None\n",
      "ComplianceAssetId: None\n",
      "Title: None\n",
      "DocumentID: 1193-QRT-2019Q2-PUBLIC\n",
      "Year: 2019\n",
      "DocumentType: Financial\n",
      "ReorgTimestamp: 2021-03-09T20:57:20\n",
      "DocumentSubType: EarningsReport\n",
      "Source: Aggredium\n",
      "MediaServiceImageTags: {}\n",
      "MediaServiceOCR: None\n",
      "ID: 50\n",
      "Created: 2024-03-17T15:42:19\n",
      "AuthorId: 1073741822\n",
      "Modified: 2024-03-17T15:42:20\n",
      "EditorId: 1073741822\n",
      "OData__CopySource: None\n",
      "CheckoutUserId: None\n",
      "OData__UIVersionString: 2.0\n",
      "GUID: 8b2b5697-79ed-4011-933e-34d0bbf436c5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print SharePoint Site Document Library Metadata\n",
    "\n",
    "def download_specific_files_for_row(row):\n",
    "    \"\"\"\n",
    "    Prints the metadata values for the first file in the '/Reorg/' document library for a given site.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        A row from the dataframe containing 'EB_SPWebUrl'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    site_url = row['EB_SPWebUrl']\n",
    "    if pd.isna(site_url):\n",
    "        return\n",
    "\n",
    "    # Use or create a CustomSharePoint instance for this site URL\n",
    "    if site_url not in sp_instances:\n",
    "        sp_instances[site_url] = CustomSharePoint(site_url=site_url)\n",
    "    sp = sp_instances[site_url]\n",
    "\n",
    "    # Construct the server-relative URL for the '/Reorg/' folder\n",
    "    parsed_url = urlparse(site_url)\n",
    "    server_relative_url = parsed_url.path.rstrip('/') + '/Reorg/'\n",
    "\n",
    "    # Get files with metadata in the folder\n",
    "    try:\n",
    "        files = sp.get_files_metadata(server_relative_url)\n",
    "        count = 0\n",
    "        for file in files:\n",
    "            list_item_properties = file.listItemAllFields.properties\n",
    "            # Print the file name\n",
    "            print(f\"File Name: {file.name}\")\n",
    "            # Print all metadata properties\n",
    "            for key, value in list_item_properties.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "            print(\"\\n\")  # Add a newline for better readability\n",
    "            count += 1\n",
    "            if count >= 1:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process files from {server_relative_url}: {e}\")\n",
    "\n",
    "        \n",
    "download_specific_files_for_row(sp_data.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ignore the below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title_x</th>\n",
       "      <th>Created_x</th>\n",
       "      <th>EB_Url</th>\n",
       "      <th>EB_UniqueID</th>\n",
       "      <th>Title_y</th>\n",
       "      <th>Created_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Antolin Q2 2023, Neutral - Reaffirmed guidance...</td>\n",
       "      <td>2023-08-01T11:35:13Z</td>\n",
       "      <td>https://c4.sharepoint.com/sites/441/SitePages/...</td>\n",
       "      <td>{0ffa3b84-2379-49e5-aedd-9697dfbd2dbd}</td>\n",
       "      <td>Antolin Q2 2023, Neutral - Reaffirmed guidance...</td>\n",
       "      <td>2023-08-01T11:35:13Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Antolin Q2 2023, Neutral - Reaffirmed guidance...</td>\n",
       "      <td>2023-08-01T12:15:17Z</td>\n",
       "      <td>https://c4.sharepoint.com/sites/441/SitePages/...</td>\n",
       "      <td>{b4132e75-1fcf-4547-93f4-91dec5d97548}</td>\n",
       "      <td>Antolin Q2 2023, Neutral - Reaffirmed guidance...</td>\n",
       "      <td>2023-08-01T12:15:17Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Balder - Q1'22 - Positive</td>\n",
       "      <td>2022-05-15T20:22:03Z</td>\n",
       "      <td>https://c4.sharepoint.com/sites/101/SitePages/...</td>\n",
       "      <td>345C69A2-6145-49D7-B411-1CB890EADB65</td>\n",
       "      <td>Balder - Q1'22 - Positive</td>\n",
       "      <td>2022-05-15T20:22:03Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Balder - Q1'22 - Positive</td>\n",
       "      <td>2022-05-16T15:34:04Z</td>\n",
       "      <td>https://c4.sharepoint.com/sites/101/SitePages/...</td>\n",
       "      <td>{345C69A2-6145-49D7-B411-1CB890EADB65}</td>\n",
       "      <td>Balder - Q1'22 - Positive</td>\n",
       "      <td>2022-05-16T15:34:04Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Banijay Q1'22 - Positive Phasing Effects, Flat...</td>\n",
       "      <td>2022-05-16T16:18:04Z</td>\n",
       "      <td>https://c4.sharepoint.com/sites/458/SitePages/...</td>\n",
       "      <td>717072A2-4E23-4070-9AA8-96B2AEDFC7BD</td>\n",
       "      <td>Banijay Q1'22 - Positive Phasing Effects, Flat...</td>\n",
       "      <td>2022-05-16T16:18:04Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Banijay Q1'22 - Positive Phasing Effects, Flat...</td>\n",
       "      <td>2022-05-16T16:26:04Z</td>\n",
       "      <td>https://c4.sharepoint.com/sites/458/SitePages/...</td>\n",
       "      <td>{717072A2-4E23-4070-9AA8-96B2AEDFC7BD}</td>\n",
       "      <td>Banijay Q1'22 - Positive Phasing Effects, Flat...</td>\n",
       "      <td>2022-05-16T16:26:04Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NH Hotels - Q1-22</td>\n",
       "      <td>2022-05-16T14:02:04Z</td>\n",
       "      <td>https://c4.sharepoint.com/sites/471/SitePages/...</td>\n",
       "      <td>7002BE19-B3C9-471D-9624-10E80B3DE298</td>\n",
       "      <td>NH Hotels - Q1-22</td>\n",
       "      <td>2022-05-16T14:02:04Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NH Hotels - Q1-22</td>\n",
       "      <td>2022-05-16T15:34:05Z</td>\n",
       "      <td>https://c4.sharepoint.com/sites/471/SitePages/...</td>\n",
       "      <td>{7002BE19-B3C9-471D-9624-10E80B3DE298}</td>\n",
       "      <td>NH Hotels - Q1-22</td>\n",
       "      <td>2022-05-16T15:34:05Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Title_x             Created_x  \\\n",
       "6  Antolin Q2 2023, Neutral - Reaffirmed guidance...  2023-08-01T11:35:13Z   \n",
       "7  Antolin Q2 2023, Neutral - Reaffirmed guidance...  2023-08-01T12:15:17Z   \n",
       "0                          Balder - Q1'22 - Positive  2022-05-15T20:22:03Z   \n",
       "2                          Balder - Q1'22 - Positive  2022-05-16T15:34:04Z   \n",
       "4  Banijay Q1'22 - Positive Phasing Effects, Flat...  2022-05-16T16:18:04Z   \n",
       "5  Banijay Q1'22 - Positive Phasing Effects, Flat...  2022-05-16T16:26:04Z   \n",
       "1                                  NH Hotels - Q1-22  2022-05-16T14:02:04Z   \n",
       "3                                  NH Hotels - Q1-22  2022-05-16T15:34:05Z   \n",
       "\n",
       "                                              EB_Url  \\\n",
       "6  https://c4.sharepoint.com/sites/441/SitePages/...   \n",
       "7  https://c4.sharepoint.com/sites/441/SitePages/...   \n",
       "0  https://c4.sharepoint.com/sites/101/SitePages/...   \n",
       "2  https://c4.sharepoint.com/sites/101/SitePages/...   \n",
       "4  https://c4.sharepoint.com/sites/458/SitePages/...   \n",
       "5  https://c4.sharepoint.com/sites/458/SitePages/...   \n",
       "1  https://c4.sharepoint.com/sites/471/SitePages/...   \n",
       "3  https://c4.sharepoint.com/sites/471/SitePages/...   \n",
       "\n",
       "                              EB_UniqueID  \\\n",
       "6  {0ffa3b84-2379-49e5-aedd-9697dfbd2dbd}   \n",
       "7  {b4132e75-1fcf-4547-93f4-91dec5d97548}   \n",
       "0    345C69A2-6145-49D7-B411-1CB890EADB65   \n",
       "2  {345C69A2-6145-49D7-B411-1CB890EADB65}   \n",
       "4    717072A2-4E23-4070-9AA8-96B2AEDFC7BD   \n",
       "5  {717072A2-4E23-4070-9AA8-96B2AEDFC7BD}   \n",
       "1    7002BE19-B3C9-471D-9624-10E80B3DE298   \n",
       "3  {7002BE19-B3C9-471D-9624-10E80B3DE298}   \n",
       "\n",
       "                                             Title_y             Created_y  \n",
       "6  Antolin Q2 2023, Neutral - Reaffirmed guidance...  2023-08-01T11:35:13Z  \n",
       "7  Antolin Q2 2023, Neutral - Reaffirmed guidance...  2023-08-01T12:15:17Z  \n",
       "0                          Balder - Q1'22 - Positive  2022-05-15T20:22:03Z  \n",
       "2                          Balder - Q1'22 - Positive  2022-05-16T15:34:04Z  \n",
       "4  Banijay Q1'22 - Positive Phasing Effects, Flat...  2022-05-16T16:18:04Z  \n",
       "5  Banijay Q1'22 - Positive Phasing Effects, Flat...  2022-05-16T16:26:04Z  \n",
       "1                                  NH Hotels - Q1-22  2022-05-16T14:02:04Z  \n",
       "3                                  NH Hotels - Q1-22  2022-05-16T15:34:05Z  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For some reason, we have some suplicate earnings note pages (4 in total)\n",
    "\n",
    "sp_data = sp_data[['Title', 'Created', 'EB_Url', 'EB_UniqueID']]\n",
    "\n",
    "# We have 3774 unique EB_UniqueID\n",
    "# We have 3770 unique EB_Url\n",
    "\n",
    "df = sp_data\n",
    "# Find rows where 'EB_UniqueID' has unique values but 'EB_Url' has duplicates\n",
    "unique_eb_uniqueid = df[~df['EB_UniqueID'].duplicated(keep=False)]\n",
    "duplicate_eb_url = df[df['EB_Url'].duplicated(keep=False)]\n",
    "\n",
    "# Merge these results to find rows where 'EB_UniqueID' is unique but 'EB_Url' is duplicated\n",
    "result = pd.merge(unique_eb_uniqueid, duplicate_eb_url, on=['EB_UniqueID', 'EB_Url'], how='inner')\n",
    "\n",
    "# Display the result\n",
    "result.sort_values(by=\"Title_x\")\n",
    "\n",
    "# Print values of first two rows\n",
    "# print(result.sort_values(by=\"Title_x\").head(2).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capfourpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
