{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for file paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "RMS_FUNDAMENTAL_SCORE_FILE = DATA_DIR / \"rms_with_fundamental_score.csv\"\n",
    "RMS_ISIN_LINK_FILE = DATA_DIR / \"isin_rms_link.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ../data/rms_with_fundamental_score.csv\n",
      "Loaded ../data/isin_rms_link.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_rms_fundamental_score = pd.read_csv(RMS_FUNDAMENTAL_SCORE_FILE)\n",
    "    print(f\"Loaded {RMS_FUNDAMENTAL_SCORE_FILE}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {RMS_FUNDAMENTAL_SCORE_FILE}\")\n",
    "    raise\n",
    "\n",
    "# Load the ISIN links data\n",
    "try:\n",
    "    df_rms_isin = pd.read_csv(RMS_ISIN_LINK_FILE)\n",
    "    print(f\"Loaded {RMS_ISIN_LINK_FILE}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {RMS_ISIN_LINK_FILE}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique RmsIds in Fundamental Scores: 631\n",
      "Unique RmsIds in ISIN Links: 398\n"
     ]
    }
   ],
   "source": [
    "# Display the number of unique RmsIds in each dataframe\n",
    "unique_rms_fundamental = df_rms_fundamental_score['RmsId'].nunique()\n",
    "unique_rms_isin = df_rms_isin['RmsId'].nunique()\n",
    "\n",
    "print(f\"Unique RmsIds in Fundamental Scores: {unique_rms_fundamental}\")\n",
    "print(f\"Unique RmsIds in ISIN Links: {unique_rms_isin}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of RmsIds with missing Prospectuses: 508\n"
     ]
    }
   ],
   "source": [
    "def find_missing_prospectuses(df, processed_dir):\n",
    "    \"\"\"\n",
    "    Identify RmsIds with missing or empty prospectus directories.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing 'RmsId'.\n",
    "        processed_dir (Path): Path to the processed data directory.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of RmsIds with missing prospectuses.\n",
    "    \"\"\"\n",
    "    missing_files = []\n",
    "    for rms_id in df['RmsId'].unique():\n",
    "        folder_path = processed_dir / str(rms_id) / \"as_expected\"\n",
    "        if not folder_path.exists() or not any(folder_path.iterdir()):\n",
    "            missing_files.append(rms_id)\n",
    "    return missing_files\n",
    "\n",
    "missing_files = find_missing_prospectuses(df_rms_fundamental_score, PROCESSED_DIR)\n",
    "print(f\"Number of RmsIds with missing Prospectuses: {len(missing_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.merge(\n",
    "    df_rms_isin,\n",
    "    df_rms_fundamental_score,\n",
    "    on=\"RmsId\",\n",
    "    how=\"left\",\n",
    "    indicator=True  # To identify unmatched rows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after cleaning: 9748 rows\n"
     ]
    }
   ],
   "source": [
    "# Ensure IssueDate and ScoringDate are datetime\n",
    "result_df['IssueDate'] = pd.to_datetime(result_df['IssueDate'])\n",
    "result_df['ScoringDate'] = pd.to_datetime(result_df['ScoringDate'])\n",
    "\n",
    "# Calculate the absolute difference between IssueDate and ScoringDate\n",
    "result_df['DateDifference'] = (result_df['IssueDate'] - result_df['ScoringDate']).abs()\n",
    "\n",
    "# Sort by 'RmsId', 'ScoringDate', and 'DateDifference'\n",
    "result_df = result_df.sort_values(\n",
    "    by=['RmsId', 'ScoringDate', 'DateDifference']\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Drop the 'DateDifference' column if not needed\n",
    "result_df = result_df.drop(columns=['DateDifference'])\n",
    "\n",
    "# Drop rows with missing 'ScoringDate'\n",
    "df_cleaned = result_df.dropna(subset=['ScoringDate']).copy()\n",
    "\n",
    "print(f\"Data after cleaning: {df_cleaned.shape[0]} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[np.int64(5),\n",
       "  [[Timestamp('2022-06-29 00:00:00'),\n",
       "    ['DE000A3LF6J0',\n",
       "     'XS2336188029',\n",
       "     'XS2283225477',\n",
       "     'XS2283224231',\n",
       "     'XS2248826294',\n",
       "     'XS2010029663',\n",
       "     'DE000A3L3AG9',\n",
       "     'DE000A3L3AE4',\n",
       "     'DE000A3L3AD6',\n",
       "     'DE000A3L3AJ3',\n",
       "     'DE000A3L3AH7',\n",
       "     'XS1843441491',\n",
       "     'DE000A2RUD79',\n",
       "     'XS1713464441',\n",
       "     'XS1713464524',\n",
       "     'XS1731858392',\n",
       "     'XS1731858715',\n",
       "     'XS1652965085']],\n",
       "   [Timestamp('2022-08-10 00:00:00'),\n",
       "    ['DE000A3LF6J0',\n",
       "     'XS2336188029',\n",
       "     'XS2283225477',\n",
       "     'XS2283224231',\n",
       "     'XS2248826294',\n",
       "     'XS2010029663',\n",
       "     'DE000A3L3AG9',\n",
       "     'DE000A3L3AE4',\n",
       "     'DE000A3L3AD6',\n",
       "     'DE000A3L3AJ3',\n",
       "     'DE000A3L3AH7',\n",
       "     'XS1843441491',\n",
       "     'DE000A2RUD79',\n",
       "     'XS1713464441',\n",
       "     'XS1713464524',\n",
       "     'XS1731858392',\n",
       "     'XS1731858715',\n",
       "     'XS1652965085']],\n",
       "   [Timestamp('2023-02-24 00:00:00'),\n",
       "    ['DE000A3LF6J0',\n",
       "     'DE000A3L3AG9',\n",
       "     'DE000A3L3AE4',\n",
       "     'DE000A3L3AD6',\n",
       "     'DE000A3L3AJ3',\n",
       "     'DE000A3L3AH7',\n",
       "     'XS2336188029',\n",
       "     'XS2283225477',\n",
       "     'XS2283224231',\n",
       "     'XS2248826294',\n",
       "     'XS2010029663',\n",
       "     'XS1843441491',\n",
       "     'DE000A2RUD79',\n",
       "     'XS1713464441',\n",
       "     'XS1713464524',\n",
       "     'XS1731858392',\n",
       "     'XS1731858715',\n",
       "     'XS1652965085']]]],\n",
       " [np.int64(13), [[Timestamp('2020-10-28 00:00:00'), ['DE000A1KQ177']]]],\n",
       " [np.int64(16),\n",
       "  [[Timestamp('2020-11-27 00:00:00'),\n",
       "    ['XS2232102876',\n",
       "     'US02156LAE11',\n",
       "     'US02156LAF85',\n",
       "     'XS2332975007',\n",
       "     'XS2138128314',\n",
       "     'US02156TAB08',\n",
       "     'XS2110799751',\n",
       "     'XS2138140798',\n",
       "     'US02156TAA25',\n",
       "     'XS2390152986',\n",
       "     'US02156LAH42',\n",
       "     'XS2053846262',\n",
       "     'XS2054539627',\n",
       "     'US02156LAC54',\n",
       "     'XS1859337419',\n",
       "     'US02156LAA98',\n",
       "     'NEW SFRFP 11.5 2027',\n",
       "     'XS2739001019',\n",
       "     'USF6628RAA17',\n",
       "     'US67054KAA79',\n",
       "     'XS1028956149',\n",
       "     'XS1028956222',\n",
       "     'US67054LAB36',\n",
       "     'US67054LAC19']]]]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def group_isins(df):\n",
    "    \"\"\"\n",
    "    Group ISINs by RmsId and ScoringDate.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Cleaned DataFrame with necessary columns.\n",
    "        \n",
    "    Returns:\n",
    "        list: List containing [RmsId, [[ScoringDate, ISINs], ...]] for each RmsId.\n",
    "    \"\"\"\n",
    "    grouped = defaultdict(list)\n",
    "    \n",
    "    for (rms_id, scoring_date), group in df.groupby(['RmsId', 'ScoringDate']):\n",
    "        ordered_isins = list(dict.fromkeys(group['ISIN'].tolist()))\n",
    "        grouped[rms_id].append([scoring_date, ordered_isins])\n",
    "    \n",
    "    # Convert defaultdict to the desired list format\n",
    "    final_result = [\n",
    "        [rms_id, scoring_date_lists]\n",
    "        for rms_id, scoring_date_lists in grouped.items()\n",
    "    ]\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "final_result = group_isins(df_cleaned)\n",
    "\n",
    "import pickle\n",
    "# Save the final result to a pickle file\n",
    "# with open(DATA_DIR / \"grouped_isins.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(final_result, f)\n",
    "\n",
    "final_result[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mester_nlp_mini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
