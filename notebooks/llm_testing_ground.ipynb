{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\mester_mini\\lib\\site-packages\\langchain_community\\llms\\llamacpp.py:140\u001b[0m, in \u001b[0;36mLlamaCpp.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama, LlamaGrammar\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaCpp\n\u001b[0;32m     34\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../gguf_folder/Llama-3.2-3B-Instruct-Q8_0.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 35\u001b[0m llm_hf \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaCpp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_gpu_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\n\u001b[0;32m     40\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# ---- 3. Build a Runnable to format the prompt ----\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# This Runnable takes a dict with keys: \"question\", \"Section_Title\", \"Section_Text\"\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# and returns the fully formatted string prompt.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m prompt_runnable \u001b[38;5;241m=\u001b[39m RunnableLambda(\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: PROMPT\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     48\u001b[0m         question\u001b[38;5;241m=\u001b[39mx[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     )\n\u001b[0;32m     52\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\mester_mini\\lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\mester_mini\\lib\\site-packages\\pydantic\\_internal\\_decorators_v1.py:148\u001b[0m, in \u001b[0;36mmake_v1_generic_root_validator.<locals>._wrapper1\u001b[1;34m(values, _)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapper1\u001b[39m(values: RootValidatorValues, _: core_schema\u001b[38;5;241m.\u001b[39mValidationInfo) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RootValidatorValues:\n\u001b[1;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\mester_mini\\lib\\site-packages\\langchain_core\\utils\\pydantic.py:219\u001b[0m, in \u001b[0;36mpre_init.<locals>.wrapper\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    216\u001b[0m             values[name] \u001b[38;5;241m=\u001b[39m field_info\u001b[38;5;241m.\u001b[39mdefault\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Call the decorated function\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pih\\AppData\\Local\\miniconda3\\envs\\mester_mini\\lib\\site-packages\\langchain_community\\llms\\llamacpp.py:142\u001b[0m, in \u001b[0;36mLlamaCpp.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama, LlamaGrammar\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import llama-cpp-python library. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install the llama-cpp-python library to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse this embedding model: pip install llama-cpp-python\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m     )\n\u001b[0;32m    148\u001b[0m model_path \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    149\u001b[0m model_param_names \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrope_freq_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrope_freq_base\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    166\u001b[0m ]\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python"
     ]
    }
   ],
   "source": [
    "# ---- 1. Your inputs & prompt ----\n",
    "\n",
    "Section_Title = \"Risk Section Title\"\n",
    "Section_Text = \"Risk Section Text\"\n",
    "\n",
    "questions_market_dynamics = {\n",
    "    \"Market Dynamics - a\": \"Does the text mention that the company is exposed to risks associated with cyclical products?\"\n",
    "}\n",
    "\n",
    "questions_intra_industry_competition = {\n",
    "    \"Intra-Industry Competition - a\": \"Does the text mention that market pricing for the company's products is irrational?\"\n",
    "}\n",
    "\n",
    "all_question_dicts = [\n",
    "    questions_market_dynamics,\n",
    "    questions_intra_industry_competition\n",
    "]\n",
    "\n",
    "PROMPT = \"\"\"{question} \n",
    "Title: {Section_Title} \n",
    "Text: {Section_Text}\n",
    "\n",
    "Provide your answer in the following JSON format:\n",
    "{{\n",
    "  \"Answer\": \"Yes\" or \"No\", \n",
    "  \"Evidence\": \"The exact sentences from the document that support your answer; otherwise, leave blank.\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# ---- 2. Load your LLM locally ----\n",
    "# Requires `pip install langchain_community`\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "model_path = \"../gguf_folder/Llama-3.2-3B-Instruct-Q8_0.gguf\"\n",
    "llm_hf = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    n_ctx=4096,\n",
    "    n_gpu_layers=35,\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "# ---- 3. Build a Runnable to format the prompt ----\n",
    "# This Runnable takes a dict with keys: \"question\", \"Section_Title\", \"Section_Text\"\n",
    "# and returns the fully formatted string prompt.\n",
    "\n",
    "prompt_runnable = RunnableLambda(\n",
    "    lambda x: PROMPT.format(\n",
    "        question=x[\"question\"],\n",
    "        Section_Title=x[\"Section_Title\"],\n",
    "        Section_Text=x[\"Section_Text\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---- 4. Pipe prompt construction into your LLM Runnable ----\n",
    "# The \"pipe\" operator (|) creates a RunnableSequence that:\n",
    "#   (a) calls prompt_runnable.invoke(...)\n",
    "#   (b) pipes its output into llm_hf.invoke(...)\n",
    "\n",
    "chain = prompt_runnable | llm_hf\n",
    "\n",
    "# ---- 5. Invoke chain for each question ----\n",
    "if __name__ == \"__main__\":\n",
    "    for question_dict in all_question_dicts:\n",
    "        for column_name, question in question_dict.items():\n",
    "            # Construct the input dict\n",
    "            input_dict = {\n",
    "                \"Section_Title\": Section_Title,\n",
    "                \"Section_Text\": Section_Text,\n",
    "                \"question\": question,\n",
    "            }\n",
    "\n",
    "            # Invoke the chain\n",
    "            output = chain.invoke(input_dict)\n",
    "            \n",
    "            # Print or store results as desired\n",
    "            print(f\"Column Name: {column_name}\")\n",
    "            print(\"LLM Output:\", output)\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mester_mini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
