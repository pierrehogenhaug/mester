{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this Notebook we extract the remaining SubScore Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code to extract unique score combinations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_with_fundamental_score = pd.read_pickle('./rms_with_fundamental_score.pkl')\n",
    "rms_with_fundamental_score.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_score_combinations = rms_with_fundamental_score[['CategoryGroup', 'Category', 'TaggedCharacteristics']].drop_duplicates()\n",
    "# Replace all types of newlines and excessive whitespace in TaggedCharacteristics\n",
    "unique_score_combinations['TaggedCharacteristics'] = unique_score_combinations['TaggedCharacteristics'].str.replace(r'[\\r\\n]+', ' ', regex=True)\n",
    "\n",
    "# Function to expand TaggedCharacteristics if it's a JSON string with multiple items\n",
    "def expand_tagged_characteristics(row):\n",
    "    try:\n",
    "        characteristics = json.loads(row['TaggedCharacteristics'])\n",
    "        if isinstance(characteristics, list):\n",
    "            # Replace newlines within each CharacteristicText\n",
    "            return pd.DataFrame([{\n",
    "                'CategoryGroup': row['CategoryGroup'],\n",
    "                'Category': row['Category'],\n",
    "                'TaggedCharacteristics': char['CharacteristicText'].replace('\\r', ' ').replace('\\n', ' '),\n",
    "                'CharacteristicInfluence': char.get('CharacteristicInfluence', None)  # Handle missing keys\n",
    "            } for char in characteristics])\n",
    "        else:\n",
    "            # If it's a single item or not a list, replace newlines if it's a string\n",
    "            if isinstance(characteristics, str):\n",
    "                characteristics = characteristics.replace('\\r', ' ').replace('\\n', ' ')\n",
    "            return pd.DataFrame([{\n",
    "                'CategoryGroup': row['CategoryGroup'],\n",
    "                'Category': row['Category'],\n",
    "                'TaggedCharacteristics': characteristics,\n",
    "                'CharacteristicInfluence': row.get('CharacteristicInfluence', None)\n",
    "            }])\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        # If parsing fails, replace newlines in the original TaggedCharacteristics\n",
    "        cleaned_text = row['TaggedCharacteristics'].replace('\\r', ' ').replace('\\n', ' ')\n",
    "        return pd.DataFrame([{\n",
    "            'CategoryGroup': row['CategoryGroup'],\n",
    "            'Category': row['Category'],\n",
    "            'TaggedCharacteristics': cleaned_text,\n",
    "            'CharacteristicInfluence': row.get('CharacteristicInfluence', None)\n",
    "        }])\n",
    "\n",
    "# Applying the function to each row and combining results\n",
    "expanded_unique_score_combinations = pd.concat(\n",
    "    unique_score_combinations.apply(expand_tagged_characteristics, axis=1).to_list(),\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Sort, drop duplicates, and save to CSV\n",
    "unique_score_combinations = expanded_unique_score_combinations.sort_values(by=['CategoryGroup', 'Category', 'CharacteristicInfluence']).drop_duplicates()\n",
    "unique_score_combinations.to_csv('unique_score_combinations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the questions and instantiating the LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the questions corresponding to each column\n",
    "questions_market_dynamics = {\n",
    "    \"Market Dynamics - a\": \"Does the text mention that the company is exposed to risks associated with cyclical products?\",\n",
    "    \"Market Dynamics - b\": \"Does the text mention risks related to demographic or structural trends affecting the market?\",\n",
    "    \"Market Dynamics - c\": \"Does the text mention risks due to seasonal volatility in the industry?\"\n",
    "}\n",
    "questions_intra_industry_competition = {\n",
    "    \"Intra-Industry Competition - a\": \"Does the text mention that market pricing for the company's products or services is irrational or not based on fundamental factors?\",\n",
    "    \"Intra-Industry Competition - b\": \"Does the text mention that the market is highly fragmented with no clear leader or that there is only one dominant leader?\",\n",
    "    \"Intra-Industry Competition - c\": \"Does the text mention low barriers to entry in the industry, making it easy for new competitors to enter the market?\"\n",
    "}\n",
    "questions_regulatory_framework = {\n",
    "    \"Regulatory Framework - a\": \"Does the text mention that the industry is subject to a high degree of regulatory scrutiny?\",\n",
    "    \"Regulatory Framework - b\": \"Does the text mention a high dependency on regulation or being a beneficiary from regulation in an unstable regulatory environment?\"\n",
    "}\n",
    "questions_technology_risk = {\n",
    "    \"Technology Risk - a\": \"Does the text mention that the industry is susceptible to rapid technological advances or innovations?\",\n",
    "    \"Technology Risk - b\": \"Does the text mention that the company is perceived as a disruptor or is threatened by emerging technological changes?\"\n",
    "}\n",
    "\n",
    "all_question_dicts = [\n",
    "    questions_market_dynamics,\n",
    "    questions_intra_industry_competition,\n",
    "    questions_regulatory_framework,\n",
    "    questions_technology_risk\n",
    "]\n",
    "\n",
    "# Original questions\n",
    "questions_market_dynamics_original = {\n",
    "    \"Market Dynamics - a\": \"Exposure to cyclical products\",\n",
    "    \"Market Dynamics - b\": \"Impact of demographic and structural trends\",\n",
    "    \"Market Dynamics - c\": \"Seasonal industry volatility\"\n",
    "}\n",
    "questions_intra_industry_competition_original = {\n",
    "    \"Intra-Industry Competition - a\": \"Market pricing has not shown to be rational\",\n",
    "    \"Intra-Industry Competition - b\": \"Highly fragmented market with no clear leader or only one leader\",\n",
    "    \"Intra-Industry Competition - c\": \"Low barriers to entry\"\n",
    "}\n",
    "questions_regulatory_framework_original = {\n",
    "    \"Regulatory Framework - a\": \"Industry has high degree of regulatory scrutiny\",\n",
    "    \"Regulatory Framework - b\": \"High dependency on regulation or is a beneficiary from regulation in an unstable regulatory environment\"\n",
    "}\n",
    "questions_technology_risk_original = {\n",
    "    \"Technology Risk - a\": \"Industry susceptibility to technological advances\",\n",
    "    \"Technology Risk - b\": \"Company viewed as a disruptee/threatened by technological change\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prospectus ID</th>\n",
       "      <th>Original Filename</th>\n",
       "      <th>Section ID</th>\n",
       "      <th>Section Title</th>\n",
       "      <th>Subsection ID</th>\n",
       "      <th>Subsection Title</th>\n",
       "      <th>Subsubsection ID</th>\n",
       "      <th>Subsubsection Title</th>\n",
       "      <th>Subsubsection Text</th>\n",
       "      <th>Market Dynamics - a</th>\n",
       "      <th>Market Dynamics - b</th>\n",
       "      <th>Market Dynamics - c</th>\n",
       "      <th>Parsing Error</th>\n",
       "      <th>Intra-Industry Competition - a</th>\n",
       "      <th>Intra-Industry Competition - b</th>\n",
       "      <th>Intra-Industry Competition - c</th>\n",
       "      <th>Regulatory Framework - a</th>\n",
       "      <th>Regulatory Framework - b</th>\n",
       "      <th>Technology Risk - a</th>\n",
       "      <th>Technology Risk - b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>235</td>\n",
       "      <td>Final Offerings 2020.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>RISK FACTORS</td>\n",
       "      <td>1.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.1.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>_An investment in the Notes involves a high de...</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>Highly Relevant: the risks described below</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>Highly Relevant: Subsubsection Title: ... and ...</td>\n",
       "      <td>Highly Relevant</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>Not Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>Final Offerings 2020.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>RISK FACTORS</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Risks Relating to the Group’s Business, Techno...</td>\n",
       "      <td>1.1.1</td>\n",
       "      <td>The Group faces significant competition in eac...</td>\n",
       "      <td>The French telecommunications market is a matu...</td>\n",
       "      <td>Highly Relevant: Various evidence throughout t...</td>\n",
       "      <td>Highly Relevant</td>\n",
       "      <td>Highly Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Highly Relevant</td>\n",
       "      <td>Highly Relevant: ...the Group also competes wi...</td>\n",
       "      <td>Highly Relevant: The exact phrases or sentence...</td>\n",
       "      <td>Highly Relevant: Several evidence are presente...</td>\n",
       "      <td>Highly Relevant</td>\n",
       "      <td>Highly Relevant: This is a highly relevant ans...</td>\n",
       "      <td>Highly Relevant: The Group also faces competit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Prospectus ID         Original Filename  Section ID Section Title  \\\n",
       "0           235  Final Offerings 2020.pdf           1  RISK FACTORS   \n",
       "1            16  Final Offerings 2020.pdf           1  RISK FACTORS   \n",
       "\n",
       "   Subsection ID                                   Subsection Title  \\\n",
       "0            1.1                                                NaN   \n",
       "1            1.1  Risks Relating to the Group’s Business, Techno...   \n",
       "\n",
       "  Subsubsection ID                                Subsubsection Title  \\\n",
       "0            1.1.1                                                NaN   \n",
       "1            1.1.1  The Group faces significant competition in eac...   \n",
       "\n",
       "                                  Subsubsection Text  \\\n",
       "0  _An investment in the Notes involves a high de...   \n",
       "1  The French telecommunications market is a matu...   \n",
       "\n",
       "                                 Market Dynamics - a  \\\n",
       "0                                       Not Relevant   \n",
       "1  Highly Relevant: Various evidence throughout t...   \n",
       "\n",
       "                          Market Dynamics - b Market Dynamics - c  \\\n",
       "0  Highly Relevant: the risks described below        Not Relevant   \n",
       "1                             Highly Relevant     Highly Relevant   \n",
       "\n",
       "   Parsing Error Intra-Industry Competition - a  \\\n",
       "0            NaN                   Not Relevant   \n",
       "1            NaN                Highly Relevant   \n",
       "\n",
       "                      Intra-Industry Competition - b  \\\n",
       "0                                       Not Relevant   \n",
       "1  Highly Relevant: ...the Group also competes wi...   \n",
       "\n",
       "                      Intra-Industry Competition - c  \\\n",
       "0                                       Not Relevant   \n",
       "1  Highly Relevant: The exact phrases or sentence...   \n",
       "\n",
       "                            Regulatory Framework - a Regulatory Framework - b  \\\n",
       "0  Highly Relevant: Subsubsection Title: ... and ...          Highly Relevant   \n",
       "1  Highly Relevant: Several evidence are presente...          Highly Relevant   \n",
       "\n",
       "                                 Technology Risk - a  \\\n",
       "0                                       Not Relevant   \n",
       "1  Highly Relevant: This is a highly relevant ans...   \n",
       "\n",
       "                                 Technology Risk - b  \n",
       "0                                       Not Relevant  \n",
       "1  Highly Relevant: The Group also faces competit...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the language model\n",
    "llm = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "# Check if the processed file exists; if not, process the raw data\n",
    "processed_file_path = '../data/prospectuses_data_processed.csv'\n",
    "raw_file_path = '../data/prospectuses_data.csv'\n",
    "\n",
    "# Check if processed file exists\n",
    "if os.path.exists(processed_file_path):\n",
    "    df = pd.read_csv(processed_file_path)\n",
    "else:\n",
    "    print(\"Processed file not found. Processing raw data...\")\n",
    "    df = pd.read_csv(raw_file_path)\n",
    "    # Filter out rows that have \"failed parsing\" in the Section ID column\n",
    "    df = df[df['Section ID'] != \"failed parsing\"]\n",
    "\n",
    "# Ensure the relevance and evidence columns are created with a compatible data type\n",
    "for question_dict in all_question_dicts:\n",
    "    # Iterate through each question key in the current dictionary\n",
    "    for column_name in question_dict.keys():\n",
    "        if column_name in df.columns:\n",
    "            df[column_name] = df[column_name].astype('string')\n",
    "        else:\n",
    "            df[column_name] = \"\"\n",
    "\n",
    "df.head(2)\n",
    "# print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fields(response):\n",
    "    # Remove any newlines and extra spaces\n",
    "    response = ' '.join(response.strip().split())\n",
    "\n",
    "    # Extract the Relevance field\n",
    "    relevance_match = re.search(r'\"Relevance\"\\s*:\\s*\"([^\"]+)\"', response)\n",
    "    if relevance_match:\n",
    "        relevance = relevance_match.group(1).strip()\n",
    "    else:\n",
    "        relevance = \"Parsing Error\"\n",
    "\n",
    "    # Extract the Evidence field(s)\n",
    "    evidence_match = re.search(r'\"Evidence\"\\s*:\\s*(.+?)(?:,?\\s*\"[^\"]+\"\\s*:|\\s*}$)', response)\n",
    "    if evidence_match:\n",
    "        evidence_str = evidence_match.group(1).strip()\n",
    "        # Remove any trailing commas or braces\n",
    "        evidence_str = evidence_str.rstrip(', }')\n",
    "        # Split the evidence_str into individual evidence items\n",
    "        # Evidence items are strings enclosed in double quotes\n",
    "        evidence_items = re.findall(r'\"([^\"]+)\"', evidence_str)\n",
    "        evidence = evidence_items\n",
    "    else:\n",
    "        evidence = []\n",
    "\n",
    "    return relevance, evidence\n",
    "\n",
    "\n",
    "def analyze_prospectus_row_single_question(row, question):\n",
    "    # System and user prompts\n",
    "    system_prompt = \"You are an expert in analyzing bond prospectuses and identifying specific risk factors.\"\n",
    "\n",
    "    # Format the user prompt using the row's data\n",
    "    prompt = f\"\"\"\n",
    "{system_prompt}\n",
    "\n",
    "For the following question and text, judge whether the text is \"Highly Relevant\", \"Somewhat Relevant\", or \"Not Relevant\".\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Text:\n",
    "Subsubsection Title: {row['Subsubsection Title']}\n",
    "Subsubsection Text: {row['Subsubsection Text']}\n",
    "\n",
    "\n",
    "Please provide your answer in the following JSON format:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"Highly Relevant\", \"Somewhat Relevant\", or \"Not Relevant\",\n",
    "  \"Evidence\": \"The exact phrases or sentences from the document that support your assessment; otherwise, leave blank.\"\n",
    "}}\n",
    "\n",
    "Note: Only provide the JSON response without any additional text.\n",
    "\"\"\"\n",
    "    # Run the prompt through the model\n",
    "    response = llm.invoke(input=prompt)\n",
    "\n",
    "    # Parse the response\n",
    "    try:\n",
    "        # Extract the Relevance and Evidence fields\n",
    "        relevance, evidence_list = extract_fields(response)\n",
    "        # Join multiple evidence items into a single string\n",
    "        evidence = '; '.join(evidence_list)\n",
    "    except Exception as e:\n",
    "        relevance = \"Parsing Error\"\n",
    "        evidence = \"\"\n",
    "\n",
    "    # Combine relevance and evidence\n",
    "    if relevance in [\"Highly Relevant\", \"Somewhat Relevant\"] and evidence:\n",
    "        combined_answer = f\"{relevance}: {evidence}\"\n",
    "    elif relevance in [\"Highly Relevant\", \"Somewhat Relevant\"]:\n",
    "        combined_answer = relevance\n",
    "    elif relevance == \"Not Relevant\":\n",
    "        combined_answer = \"Not Relevant\"\n",
    "    else:\n",
    "        combined_answer = \"Parsing Error\"\n",
    "\n",
    "    # For debugging\n",
    "    if combined_answer == \"Parsing Error\":\n",
    "        print(\"Parsing Error encountered. Response was:\")\n",
    "        print(response)\n",
    "\n",
    "    return combined_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the LLM Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:  29%|██▉       | 2330/7952 [03:00<6:00:57,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 new rows. Pausing for 30 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:  29%|██▉       | 2340/7952 [06:17<26:22:42, 16.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 new rows. Pausing for 30 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:  30%|██▉       | 2350/7952 [10:28<36:00:22, 23.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 new rows. Pausing for 30 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:  30%|██▉       | 2360/7952 [13:16<21:27:43, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 new rows. Pausing for 30 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:  30%|██▉       | 2370/7952 [16:47<24:38:12, 15.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 new rows. Pausing for 30 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:  30%|██▉       | 2380/7952 [20:10<32:12:28, 20.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 new rows. Pausing for 30 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:  30%|███       | 2390/7952 [24:11<31:49:09, 20.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 new rows. Pausing for 30 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:  30%|███       | 2400/7952 [27:48<28:53:53, 18.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 new rows. Pausing for 30 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:  30%|███       | 2410/7952 [30:53<26:32:03, 17.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 new rows. Pausing for 30 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:  30%|███       | 2420/7952 [35:11<31:07:09, 20.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 new rows. Pausing for 30 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:  31%|███       | 2430/7952 [38:46<26:57:31, 17.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 new rows. Pausing for 30 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:  31%|███       | 2440/7952 [43:19<25:55:56, 16.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 new rows. Pausing for 30 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:  31%|███       | 2450/7952 [46:53<28:11:49, 18.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 new rows. Pausing for 30 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:  31%|███       | 2460/7952 [50:31<33:10:24, 21.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 new rows. Pausing for 30 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:  31%|███       | 2470/7952 [55:35<52:22:25, 34.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 new rows. Pausing for 30 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:  31%|███       | 2480/7952 [59:38<45:32:12, 29.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 new rows. Pausing for 30 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:  31%|███▏      | 2490/7952 [1:05:09<51:01:09, 33.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 new rows. Pausing for 30 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:  31%|███▏      | 2490/7952 [1:05:27<2:23:34,  1.58s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m         df\u001b[38;5;241m.\u001b[39mto_csv(processed_file_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Save before sleeping\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed 10 new rows. Pausing for 30 seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m         new_rows_processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Reset counter\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Save the final DataFrame after processing all rows\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Initialize counter for new rows processed\n",
    "new_rows_processed = 0\n",
    "\n",
    "# Iterate over each row in the DataFrame with a progress bar\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing Rows\"):\n",
    "    row_processed = False  # Flag to check if we processed any new data in this row\n",
    "\n",
    "    for question_dict in all_question_dicts:\n",
    "        for column_name, question in question_dict.items():\n",
    "            # Check if the answer column is already filled\n",
    "            if pd.notnull(df.at[index, column_name]) and df.at[index, column_name] != \"\":\n",
    "                # Skip processing this row for this question\n",
    "                continue\n",
    "            combined_answer = analyze_prospectus_row_single_question(row, question)\n",
    "            df.at[index, column_name] = combined_answer\n",
    "            row_processed = True  # We processed new data in this row\n",
    "\n",
    "    if row_processed:\n",
    "        new_rows_processed += 1\n",
    "\n",
    "    # Save progress every 50 rows\n",
    "    if (index + 1) % 50 == 0:\n",
    "        df.to_csv(processed_file_path, index=False)\n",
    "        # print(f\"Progress saved at row {index + 1}\")\n",
    "\n",
    "    # After processing 10 new rows, sleep for 30 seconds\n",
    "    if new_rows_processed >= 10:\n",
    "        df.to_csv(processed_file_path, index=False)  # Save before sleeping\n",
    "        print(f\"Processed 10 new rows. Pausing for 30 seconds.\")\n",
    "        time.sleep(30)\n",
    "        new_rows_processed = 0  # Reset counter\n",
    "\n",
    "\n",
    "# Save the final DataFrame after processing all rows\n",
    "df.to_csv(processed_file_path, index=False)\n",
    "print(\"All rows have been processed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mester_nlp_mini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
