{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "notebook_dir = os.getcwd()  # Current working directory of the notebook\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from src.analysis.prospectus_analyzer import ProspectusAnalyzer\n",
    "\n",
    "DATA_DIR = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "!huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Hugging Face LLM\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "# Create a text-generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if device.type == \"mps\" else -1,\n",
    "    max_length=2048,\n",
    "    temperature=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM with the pipeline\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Initialize the analyzer with the new LLM\n",
    "analyzer = ProspectusAnalyzer(llm_model=llm)\n",
    "\n",
    "# Load the data\n",
    "raw_file_path = DATA_DIR / \"prospectuses_data.csv\"\n",
    "\n",
    "# Check if the raw file exists\n",
    "if os.path.exists(raw_file_path):\n",
    "    df_LLM = pd.read_csv(raw_file_path)\n",
    "    # Filter out rows that have \"failed parsing\" in the Section ID column\n",
    "    df_LLM = df_LLM[df_LLM['Section ID'] != \"failed parsing\"]\n",
    "else:\n",
    "    print(\"Raw data file not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model data type (fp precision)\n",
    "print(f\"Model data type (precision): {model.dtype}\")\n",
    "\n",
    "# Estimate memory footprint\n",
    "param_size = sum(p.numel() for p in model.parameters())  # Total number of parameters\n",
    "\n",
    "param_memory = param_size * torch.tensor([], dtype=model.dtype).element_size()  # Memory in bytes\n",
    "\n",
    "param_memory_mb = param_memory / (1024 ** 2)  # Memory in MB\n",
    "\n",
    "print(f\"Model parameter count: {param_size}\")\n",
    "\n",
    "print(f\"Estimated memory footprint: {param_memory_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the questions\n",
    "questions_market_dynamics = {\n",
    "    \"Market Dynamics - a\": \"Does the text mention that the company is exposed to risks associated with cyclical products?\",\n",
    "    \"Market Dynamics - b\": \"Does the text mention risks related to demographic or structural trends affecting the market?\",\n",
    "    \"Market Dynamics - c\": \"Does the text mention risks due to seasonal volatility in the industry?\"\n",
    "}\n",
    "\n",
    "specified_columns = list(questions_market_dynamics.keys())\n",
    "\n",
    "# Ensure the columns exist in the dataframe\n",
    "for column_name in specified_columns:\n",
    "    if column_name not in df_LLM.columns:\n",
    "        df_LLM[column_name] = \"\"\n",
    "    df_LLM[column_name] = df_LLM[column_name].astype(str)\n",
    "    \n",
    "# For testing, let's process a few rows\n",
    "num_rows_to_test = 3  # Adjust this number as needed\n",
    "\n",
    "# Process and display the outputs\n",
    "for index, row in df_LLM.head(num_rows_to_test).iterrows():\n",
    "    print(f\"\\nProcessing row {index}...\\n\")\n",
    "    for column_name, question in questions_market_dynamics.items():\n",
    "        # Process the question\n",
    "        combined_answer = analyzer.analyze_row_single_question(row, question)\n",
    "        df_LLM.at[index, column_name] = combined_answer\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Answer: {combined_answer}\\n\")\n",
    "\n",
    "# Display the DataFrame with the new columns\n",
    "df_LLM.head(num_rows_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mester_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
